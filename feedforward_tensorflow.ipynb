{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visual Studio Code is highly recommended to open this notebook. I used KaTeX equations in Markdown for writing equations and it might not rendered very well using other than Visual Studio Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TensorFlow implementation of a simple feedforward network as seen at Figure 21.3 in `Russell S. J. & Norvig P. (2020). Artificial intelligence : a modern approach (4th ed.). Pearson.` book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 21.3](images/fig_21_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write an expression for the output of that network as follows (taken from Equation 21.2 of the book)\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\^{y} &= g_5(in_5) \\\\\n",
    "&= g_5(w_{0,5} + w_{3,5}a_3 + w_{4,5}a_4) \\\\\n",
    "&= g_5(w_{0,5} + w_{3,5}g_3(in_3) + w_{4,5}g_4(in_4)) \\\\\n",
    "&= g_5(w_{0,5} + w_{3,5}g_3(w_{0,3} + w_{1,3}x_1 + w_{2,3}x_2)\n",
    "+ w_{4,5}g_4(w_{0,4} + w_{1,4}x_1 + w_{2,4}x_2))\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the activation functions of $g_3$ and $g_4$ are using a ReLU function, and $g_5$ is just a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 10:10:23.109997: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-29 10:10:23.111533: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-29 10:10:23.142274: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-29 10:10:23.143084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-29 10:10:23.715856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForward(tf.keras.Model):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        initializer = tf.keras.initializers.Zeros()\n",
    "        self.fc1 = tf.keras.layers.Dense(2, activation=\"relu\", name=\"fc1\", kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(1, name=\"fc2\", kernel_initializer=initializer)\n",
    "    \n",
    "    def call(self, x):\n",
    "        v = self.fc1(x)\n",
    "        z = self.fc2(v)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 10:10:25.238420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-29 10:10:25.238641: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "net = SimpleFeedForward()\n",
    "# initialize the weights by the kernel initializer\n",
    "_ = net(tf.constant([[0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we kept the inital weights random.\n",
    "But for the sake of simplicity of our study, lets initialize the weights with easy numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 [array([[0., 2.],\n",
      "       [1., 3.]], dtype=float32), array([4., 5.], dtype=float32)] \n",
      "\n",
      "fc2 [array([[6.],\n",
      "       [7.]], dtype=float32), array([8.], dtype=float32)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_v = 0\n",
    "for layer in net.layers:\n",
    "    new_weights = []\n",
    "    for i, w in enumerate(layer.get_weights()):\n",
    "        p_numel = w.size\n",
    "        nw = np.arange(last_v, last_v+p_numel, dtype=np.float32).reshape(w.shape, order=\"F\")\n",
    "        # ‘F’ means to read / write the elements using Fortran-like index order,\n",
    "        # with the first index changing fastest, and the last index changing slowest.\n",
    "        new_weights.append(nw)\n",
    "        last_v += p_numel\n",
    "    layer.set_weights(new_weights)\n",
    "    print(layer.name, layer.get_weights(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let a training example below is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([[-2, 1]], dtype=tf.float32)\n",
    "y = tf.Variable([[64]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make predictions for our training example. This process is also usually called as forward-pass/forward-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[66.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_hat = net(x)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is $66$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually calculate the output of our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 21.3b](images/fig_21_3b.png)\n",
    "\n",
    "![Forward-pass of Figure 21.3b](images/fig_21_3b_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is same at $66$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward-pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate the gradient for the\n",
    "network with respect to our previous single training example $(\\mathbf{x},y)$. (For multiple\n",
    "examples, the gradient is just the sum of the gradients for the individual examples.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the squared loss function $L_2$ is used.\n",
    "\n",
    "$$ L_2 = (y-\\^{y})^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually compute the gradient of the loss with respect to (w.r.t.) the weights using the chain rule \n",
    "$$ {dy \\over dx} = {dy \\over du}{du \\over dx}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the gradient of our $L_2$ loss w.r.t. $w_{3,5}$ should be\n",
    "$$\n",
    "{\\partial L_2 \\over \\partial w_{3,5}} = {\\partial L_2 \\over \\partial \\^{y}}{\\partial \\^{y} \\over \\partial in_5}{\\partial in_5 \\over \\partial w_{3,5}}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "{\\partial L_2 \\over \\partial \\^{y}} = {\\partial \\over \\partial \\^{y}}(y-\\^{y})^2 = 2(y − \\^{y})(-1) = −2(y − \\^{y}),\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "{\\partial \\^{y} \\over \\partial in_5} = {\\partial \\over \\partial in_5}(g_5(in_5)) = g_{5}'(in_5).\n",
    "$$\n",
    "\n",
    "Since $w_{0,5}$ and $w_{4,5}a_4$ do not depend on $w_{3,5}$, also $a_3$ does not depend on $w_{3,5}$,\n",
    "$$ {\\partial in_5 \\over \\partial w_{3,5}} = {\\partial \\over \\partial w_{3,5}}(w_{0,5} + w_{3,5}a_3 + w_{4,5}a_4) = a_3.\n",
    "$$\n",
    "\n",
    "Finally, we have \n",
    "$$\n",
    "{\\partial L_2 \\over \\partial w_{3,5}} = −2(y − \\^{y}) g_{5}'(in_5) a_3.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to compute the gradient of our $L_2$ loss w.r.t. $w_{3,5}$ using that equation for our previous training example. \n",
    "\n",
    "Since $g_5$ is just a linear function $g_5(in_5)=in_5$, then $g_{5}'(in_5)=1$, so\n",
    "$$\n",
    "{\\partial L_2 \\over \\partial w_{3,5}} = −2(64 − 66) \\cdot 1 \\cdot 5 = 20.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can update our $w_{3,5}$ (with learning rate $\\alpha=1.0$)\n",
    "$$\n",
    "w_{3,5} \\colonequals w_{3,5} - \\alpha {\\partial L_2 \\over \\partial w_{3,5}} = 6 - 1 \\cdot 20 = -14.\n",
    "$$\n",
    "The updated weight of $w_{3,5}$ is $-14$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try a slighty more difficult case, the gradient of our $L_2$ loss w.r.t. $w_{1,3}$,\n",
    "$$\n",
    "{\\partial L_2 \\over \\partial w_{1,3}} = {\\partial L_2 \\over \\partial \\^{y}}{\\partial \\^{y} \\over \\partial in_5}{\\partial in_5 \\over \\partial in_3}{\\partial in_3 \\over \\partial w_{1,3}}.\n",
    "$$\n",
    "\n",
    "As we can see, the first few steps are identical, so we can use our previous derived functions, so\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "{\\partial L_2 \\over \\partial w_{1,3}} &= −2(y − \\^{y}) g_{5}'(in_5){\\partial \\over \\partial in_3}(w_{3,5}a_3){\\partial in_3 \\over \\partial w_{1,3}} \\\\\n",
    "\n",
    "&= −2(y − \\^{y}) g_{5}'(in_5)w_{3,5}{\\partial \\over \\partial in_3}g_3(in_3){\\partial \\over \\partial w_{1,3}}(w_{0,3}+w_{1,3}x_1+w_{2,3}x_2) \\\\\n",
    "\n",
    "&= −2(y − \\^{y}) g_{5}'(in_5)w_{3,5}g_{3}'(in_3)x_1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplification in the last line because $w_{0,3}$ and $w_{2,3}x_2$ do not depend on $w_{1,3}$, also $x_1$ does not depend on any others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to compute the gradient of our $L_2$ loss w.r.t. $w_{1,3}$ using that equation for our previous training example. \n",
    "\n",
    "The $g_3$ is a rectified linear function\n",
    "$$\n",
    "g_3(in_3)=\\begin{cases}\n",
    "   in_3 &\\text{if } in_3 >= 0 \\\\\n",
    "   0 &\\text{if } in_3 < 0\n",
    "\\end{cases}\n",
    "\\\\\\enspace\\\\\n",
    "g_{3}'(in_3)=\\begin{cases}\n",
    "   1 &\\text{if } in_3 >= 0 \\\\\n",
    "   0 &\\text{if } in_3 < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So,\n",
    "$$\n",
    "{\\partial L_2 \\over \\partial w_{1,3}} = −2(64 − 66) \\cdot 1 \\cdot 6 \\cdot 1 \\cdot (-2) = -48.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can update our $w_{1,3}$ (with learning rate $\\alpha=1.0$)\n",
    "$$\n",
    "w_{1,3} \\colonequals w_{1,3} - \\alpha {\\partial L_2 \\over \\partial w_{1,3}} = 0 - 1 \\cdot (-48) = 48.\n",
    "$$\n",
    "The updated weight of $w_{1,3}$ is $48$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was... pretty tedious, huh?\n",
    "\n",
    "No worries! We can compute such gradients by **automatic differentiation** method, which applies the rules of calculus in a systematic way.\n",
    "\n",
    "In our study, we will continue using TensorFlow. Let's do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use mean squared error loss function. The \"mean\" term is doesn't matter in our case, since we use only a single training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do forward propagation and compute the loss. Also, track the operations during the forward propagation using `tf.GradientTape` (required for computing gradients, the default is the variables are not tracked). See [Automatic differentation TensorFlow guide](https://www.tensorflow.org/guide/autodiff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y_hat = net(x, training=True)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do backward-pass/back-propagation to compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = tape.gradient(loss, net.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
      "array([[-48., -56.],\n",
      "       [ 24.,  28.]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([24., 28.], dtype=float32)>, <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
      "array([[20.],\n",
      "       [16.]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use stochastic gradient descent (SGD) for updating our network parameters (weights). The \"stochastic\" term is doesn't matter in our case, since we use only a single training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we adjust/update the weights of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.apply_gradients(zip(gradients, net.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our updated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 [array([[ 48.,  58.],\n",
      "       [-23., -25.]], dtype=float32), array([-20., -23.], dtype=float32)] \n",
      "\n",
      "fc2 [array([[-14.],\n",
      "       [ -9.]], dtype=float32), array([4.], dtype=float32)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in net.layers:\n",
    "    print(layer.name, layer.get_weights(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the updated weight are the same with our manual calculation, where $w_{3,5}$ is $-14$ and $w_{1,3}$ is $48$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can freely doing experimentation on different network structures, activation functions, loss functions, and forms of composition without having to do lots of calculus to derive a new learning algorithm for each experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ai_class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45806f2a31fc2394908bf2aae38bc8f96498b1e9c39d8308e884e6256764b6c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
